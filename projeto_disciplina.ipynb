{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99bffca2-c299-45f4-ae0d-643a970702b0",
   "metadata": {},
   "source": [
    "# Projeto Prático — Web Mining & Crawler Scraping\n",
    "## Pipeline de Web Scraping para Banco Analítico Financeiro\n",
    "\n",
    "## Contexto\n",
    "Você foi contratado como **Engenheiro de Dados** por uma **fintech de investimentos** que precisa enriquecer sua base analítica com informações externas do mercado.  \n",
    "A diretoria solicitou a construção de um **pipeline de dados** que colete informações públicas, as organize e as disponibilize em um **banco analítico local** para posterior exploração via SQL e dashboards.\n",
    "\n",
    "O desafio deve ser resolvido **individualmente**, simulando uma entrega profissional para a empresa.\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivo\n",
    "Construir um **pipeline de coleta, transformação e carga (ETL)** que una **web scraping** (obrigatório) e, opcionalmente, **API pública** para compor uma base analítica com **múltiplos tipos de dados**, incluindo **séries históricas** e **notícias**.  \n",
    "\n",
    "O pipeline deve ser implementado em **Python 3.9+**, rodar em **Jupyter Notebook** e persistir os dados em um **banco analítico local** (DuckDB).\n",
    "\n",
    "---\n",
    "\n",
    "## Stack obrigatória\n",
    "- **Linguagem:** Python 3.9+  \n",
    "- **Coleta:** `requests` + `BeautifulSoup` (HTML) e **Selenium** (quando necessário para páginas dinâmicas)  \n",
    "- **Banco analítico:** DuckDB (`.duckdb`), SQLite (`.sqlite`) e/ou parquet.  \n",
    "- **Ambiente/execução:** Jupyter Notebook como entrega principal, `requirements.txt` com versões fixas  \n",
    "\n",
    "---\n",
    "\n",
    "## Escopo mínimo dos dados\n",
    "- **Notícias**: coletar pelo menos **100 notícias válidas** (título, data/hora, URL, lead/primeiro parágrafo).  \n",
    "- **Séries históricas**: coletar pelo menos **6 meses contínuos** de dados (preços, câmbio, juros, commodities ou indicadores macro).  \n",
    "- **Banco**: criar e armazenar em **≥ 3 tabelas** (exemplo: `instruments`, `prices`, `news`).  \n",
    "- **Fontes**: é permitido usar API pública como complemento, mas **não pode ser apenas API**. Pelo menos **um scraper HTML** deve ser desenvolvido.  \n",
    "\n",
    "---\n",
    "\n",
    "## Entregáveis\n",
    "- **Jupyter Notebook** executável com todas as etapas do pipeline (coleta → transformação → carga).  \n",
    "- **Arquivos auxiliares**: banco gerado (`.duckdb` ou `.sqlite`) e/ou extratos em `.parquet`.  \n",
    "- **requirements.txt** com versões fixas e, se necessário, `.env` para variáveis de ambiente.  \n",
    "- Logs de execução no notebook, mostrando contagem de itens coletados e status das etapas.  \n",
    "- Breve explicação inicial no notebook, em markdown, descrevendo objetivo, fontes e estrutura do banco.  \n",
    "- **Execução reprodutível** – notebook funcional com `requirements.txt`.  \n",
    "- **Entrega organizada e no prazo** – notebook + banco `.duckdb`/`.sqlite` (ou `.parquet`), logs e explicação inicial.  \n",
    "\n",
    "---\n",
    "\n",
    "## Critérios de Avaliação — 8 pontos\n",
    "\n",
    "#### Etapa 1 – Coleta (4 pontos)\n",
    "1. **Scraper de notícias (2 pontos)** – coleta de ≥ 100 notícias válidas.  \n",
    "2. **Dados estruturados e séries históricas (2 pontos)** – ≥ 3 meses de dados contínuos coletados.  \n",
    "\n",
    "#### Etapa 2 – Transformação (2 pontos)\n",
    "4. **Validações de dados (1 ponto)** – padronização de tipos, datas válidas, valores consistentes, URLs corretas.  \n",
    "5. **Deduplicação (1 ponto)** – regra de dedupe implementada (hash, chave composta ou unique key).  \n",
    "\n",
    "#### Etapa 3 – Carga (2 pontos)\n",
    "6. **Modelagem em banco analítico (3 pontos)** – uso de DuckDB ou SQLite com ≥ 3 tabelas relacionadas.  \n",
    "\n",
    "---\n",
    "### Prazo\n",
    " - **Prazo para submissão**:24/10/2025\n",
    " - **Tolerância:**:26/10/2025 com penalidade de -2 pontos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38874625-ee4e-41e6-840a-91c6ad4a3f38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-machine]",
   "language": "python",
   "name": "conda-env-.conda-machine-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
