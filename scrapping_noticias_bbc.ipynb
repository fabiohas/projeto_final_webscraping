{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22b3eb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install playwright\n",
    "# playwright install\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "from typing import List, Dict, Optional\n",
    "from playwright.async_api import async_playwright, Page\n",
    "from urllib.parse import urljoin\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47e14369",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:49: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/var/folders/t7/8bsz9qln153g4f9t7vn58yd40000gn/T/ipykernel_57356/3490836112.py:49: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  let title = (a.querySelector('h3,h2')?.textContent || a.textContent || '').trim().replace(/\\s+/g,' ');\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ between heading/pagination: 9 of 41\n",
      "Página 1 → 9 itens\n",
      "→ between heading/pagination: 21 of 41\n",
      "Página 2 → 21 itens\n",
      "→ between heading/pagination: 21 of 41\n",
      "Página 2 → 21 itens\n",
      "→ between heading/pagination: 22 of 41\n",
      "Página 3 → 22 itens\n",
      "→ between heading/pagination: 22 of 41\n",
      "Página 3 → 22 itens\n",
      "→ between heading/pagination: 22 of 41\n",
      "Página 4 → 22 itens\n",
      "→ between heading/pagination: 22 of 41\n",
      "Página 4 → 22 itens\n",
      "→ between heading/pagination: 22 of 41\n",
      "Página 5 → 22 itens\n",
      "→ between heading/pagination: 22 of 41\n",
      "Página 5 → 22 itens\n",
      "→ between heading/pagination: 22 of 41\n",
      "Página 6 → 22 itens\n",
      "→ between heading/pagination: 22 of 41\n",
      "Página 6 → 22 itens\n",
      "→ between heading/pagination: 22 of 41\n",
      "Página 7 → 22 itens\n",
      "→ between heading/pagination: 22 of 41\n",
      "Página 7 → 22 itens\n",
      "→ between heading/pagination: 22 of 41\n",
      "Página 8 → 22 itens\n",
      "→ between heading/pagination: 22 of 41\n",
      "Página 8 → 22 itens\n",
      "→ between heading/pagination: 22 of 41\n",
      "Página 9 → 22 itens\n",
      "→ between heading/pagination: 22 of 41\n",
      "Página 9 → 22 itens\n",
      "→ between heading/pagination: 22 of 41\n",
      "Página 10 → 22 itens\n",
      "→ between heading/pagination: 22 of 41\n",
      "Página 10 → 22 itens\n"
     ]
    }
   ],
   "source": [
    "import asyncio, csv, re\n",
    "from datetime import datetime\n",
    "from urllib.parse import urljoin\n",
    "from playwright.async_api import async_playwright, Page\n",
    "\n",
    "START_URL = \"https://www.bbc.com/news/topics/c2vdnvdg6xxt\"\n",
    "TARGET = 100\n",
    "OUTCSV = \"bbc_israel_gaza_noticias.csv\"\n",
    "BASE = \"https://www.bbc.com\"\n",
    "\n",
    "def abs_url(href): return urljoin(BASE, href or \"\")\n",
    "\n",
    "async def accept_cookies(page: Page):\n",
    "    for sel in (\n",
    "        '[data-testid=\"cookie-banner\"] button:has-text(\"Accept\")',\n",
    "        'button:has-text(\"I Agree\")','button:has-text(\"Agree\")','#bbccookies-continue-button',\n",
    "    ):\n",
    "        b = page.locator(sel).first\n",
    "        if await b.count() and await b.is_visible():\n",
    "            await b.click(); break\n",
    "\n",
    "async def wait_heading(page: Page):\n",
    "    await page.wait_for_selector('h2[data-testid=\"alaska-title\"]', timeout=15000)\n",
    "\n",
    "async def extract_latest_updates_on_page(page: Page):\n",
    "    # Extrator geométrico: pega links /news/ VISUALMENTE entre o heading e a paginação\n",
    "    js = \"\"\"\n",
    "    () => {\n",
    "      const BASE = 'https://www.bbc.com';\n",
    "      const head = document.querySelector('h2[data-testid=\"alaska-title\"]');\n",
    "      if (!head) return {items: [], debug: {reason: 'no heading'}};\n",
    "      const headBottom = head.getBoundingClientRect().bottom + window.scrollY;\n",
    "\n",
    "      // tenta achar a barra de paginação (nav ou container com botões 1,2,3)\n",
    "      let pag = document.querySelector('nav[aria-label*=\"Pagination\" i]') ||\n",
    "                Array.from(document.querySelectorAll('nav, div, section'))\n",
    "                  .find(n => /Go to page/i.test(n.textContent||'') || /\\b1\\b.*\\b2\\b.*\\b3\\b/.test(n.textContent||''));\n",
    "      let pagTop = Infinity;\n",
    "      if (pag) pagTop = pag.getBoundingClientRect().top + window.scrollY;\n",
    "\n",
    "      const links = Array.from(document.querySelectorAll('a[href*=\"/news/\"]'));\n",
    "      const filtered = [];\n",
    "\n",
    "      for (const a of links) {\n",
    "        const r = a.getBoundingClientRect();\n",
    "        const y = r.top + window.scrollY;\n",
    "        if (y > headBottom && y < pagTop) {\n",
    "          // título\n",
    "          let title = (a.querySelector('h3,h2')?.textContent || a.textContent || '').trim().replace(/\\s+/g,' ');\n",
    "          if (!title || title.length < 5) continue;\n",
    "\n",
    "          // item container p/ achar resumo/time\n",
    "          const container = a.closest('li, article, div[role=\"listitem\"], div, section') || a;\n",
    "          const p = container.querySelector('p');\n",
    "\n",
    "          const summary = (p?.textContent || '').trim().replace(/\\s+/g,' ');\n",
    "\n",
    "          try {\n",
    "            const url = new URL(a.getAttribute('href'), BASE).toString();\n",
    "            filtered.push({\"titulo\": title, \"url\": url, \"mini_resumo\": summary});\n",
    "          } catch {}\n",
    "        }\n",
    "      }\n",
    "\n",
    "      // dedupe por URL e remove duplicados do mesmo título\n",
    "      const seen = new Set();\n",
    "      const items = [];\n",
    "      for (const it of filtered) {\n",
    "        if (!seen.has(it[\"url\"])) { seen.add(it[\"url\"]); items.push(it); }\n",
    "      }\n",
    "      return {items, debug: {headBottom, pagTop, totalLinks: links.length, kept: items.length}};\n",
    "    }\n",
    "    \"\"\"\n",
    "    res = await page.evaluate(js)\n",
    "    items = res[\"items\"]\n",
    "    iso = datetime.now().isoformat()\n",
    "    for it in items: it[\"timestamp_coleta\"] = iso\n",
    "    print(f'→ between heading/pagination: {res[\"debug\"][\"kept\"]} of {res[\"debug\"][\"totalLinks\"]}')\n",
    "    return items\n",
    "\n",
    "async def get_max_page(page: Page) -> int:\n",
    "    # lê todos \"Go to page N\" e pega o maior\n",
    "    nums = set()\n",
    "    btns = page.locator('button[aria-label^=\"Go to page \"]')\n",
    "    for i in range(await btns.count()):\n",
    "        lbl = await btns.nth(i).get_attribute(\"aria-label\")\n",
    "        m = re.search(r\"(\\d+)$\", lbl or \"\")\n",
    "        if m: nums.add(int(m.group(1)))\n",
    "    # fallback: números visíveis no paginador\n",
    "    nav = page.locator(\"nav\").filter(has_text=re.compile(r\"\\b1\\b\"))\n",
    "    if await nav.count():\n",
    "        txt = \" \".join(await nav.first.all_text_contents())\n",
    "        for n in re.findall(r\"\\b\\d+\\b\", txt):\n",
    "            nums.add(int(n))\n",
    "    return max(nums) if nums else 1\n",
    "\n",
    "async def click_page_n(page: Page, n: int) -> bool:\n",
    "    # garante que o paginador está na tela\n",
    "    await page.mouse.wheel(0, 99999)\n",
    "    # 1) força o clique via JS no aria-label \"Go to page n\"\n",
    "    ok = await page.evaluate(\"\"\"\n",
    "    (n) => {\n",
    "      const byAria = Array.from(document.querySelectorAll('button[aria-label^=\"Go to page \"]'))\n",
    "        .find(b => (b.getAttribute('aria-label')||'').trim().endsWith(String(n)));\n",
    "      if (byAria) { byAria.click(); return true; }\n",
    "      // fallback por texto visível = n\n",
    "      const byText = Array.from(document.querySelectorAll('nav button, button'))\n",
    "        .find(b => (b.textContent||'').trim() === String(n));\n",
    "      if (byText) { byText.click(); return true; }\n",
    "      return false;\n",
    "    }\n",
    "    \"\"\", n)\n",
    "    if ok:\n",
    "        await page.wait_for_load_state(\"domcontentloaded\")\n",
    "        await page.wait_for_timeout(900)\n",
    "        return True\n",
    "\n",
    "    # 2) fallback: clica no chevron \">\" (próxima)\n",
    "    chevron = page.locator('nav button[aria-label*=\"next\" i], nav button:has-text(\">\"), nav button:has-text(\"›\")').first\n",
    "    if await chevron.count():\n",
    "        await chevron.click()\n",
    "        await page.wait_for_load_state(\"domcontentloaded\")\n",
    "        await page.wait_for_timeout(900)\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "async def scrape_latest_updates(target: int = TARGET):\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        context = await browser.new_context()\n",
    "        page = await context.new_page()\n",
    "        await page.goto(START_URL, timeout=60_000)\n",
    "        await accept_cookies(page)\n",
    "        await wait_heading(page)\n",
    "\n",
    "        results, seen = [], set()\n",
    "        page_no = 1\n",
    "        max_page = await get_max_page(page)\n",
    "\n",
    "        while len(results) < target and page_no <= max_page:\n",
    "            # rola um pouco p/ garantir render\n",
    "            await page.mouse.wheel(0, 2200); await asyncio.sleep(0.3)\n",
    "            batch = await extract_latest_updates_on_page(page)\n",
    "            print(f\"Página {page_no} → {len(batch)} itens\")\n",
    "            for it in batch:\n",
    "                if it[\"url\"] in seen: continue\n",
    "                seen.add(it[\"url\"]); results.append(it)\n",
    "                if len(results) >= target: break\n",
    "            if len(results) >= target: break\n",
    "            page_no += 1\n",
    "            if page_no > max_page: break\n",
    "            # leva o paginador ao viewport e clica no número\n",
    "            await page.mouse.wheel(0, 9_999); await asyncio.sleep(0.2)\n",
    "            if not await click_page_n(page, page_no):\n",
    "                print(f\"Não consegui clicar na página {page_no}\")\n",
    "                break\n",
    "\n",
    "        await browser.close()\n",
    "        return results[:target]\n",
    "\n",
    "async def save_csv(rows, path=OUTCSV):\n",
    "    cols = [\"titulo\",\"url\",\"mini_resumo\",\"timestamp_coleta\"]\n",
    "    with open(path,\"w\",newline=\"\",encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=cols); w.writeheader()\n",
    "        for r in rows: w.writerow({k:r.get(k,\"\") for k in cols})\n",
    "\n",
    "# Jupyter:\n",
    "data = await scrape_latest_updates(100)\n",
    "await save_csv(data, OUTCSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5202693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titulo</th>\n",
       "      <th>url</th>\n",
       "      <th>mini_resumo</th>\n",
       "      <th>timestamp_coleta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Israel confirms identities of hostages' bodies...</td>\n",
       "      <td>https://www.bbc.com/news/articles/c4gj90j2g8jo</td>\n",
       "      <td>The bodies were identified as those of Amiram ...</td>\n",
       "      <td>2025-10-31T20:25:53.516424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who are the released hostages?</td>\n",
       "      <td>https://www.bbc.com/news/articles/cpvl9k4mw8no</td>\n",
       "      <td>The Israeli military says 20 living hostages h...</td>\n",
       "      <td>2025-10-31T20:25:53.516424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UK pledges £4m to clear land mines to help flo...</td>\n",
       "      <td>https://www.bbc.com/news/articles/c9d6x02xdj6o</td>\n",
       "      <td>It will allow a UN body to clear land mines an...</td>\n",
       "      <td>2025-10-31T20:25:53.516424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dependants of some Gazan students can join the...</td>\n",
       "      <td>https://www.bbc.com/news/articles/cly91lj9y47o</td>\n",
       "      <td>The decision is a reversal of the original pol...</td>\n",
       "      <td>2025-10-31T20:25:53.516424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Can the Gaza ceasefire deal survive?</td>\n",
       "      <td>https://www.bbc.com/news/articles/ckgk4x5ze3mo</td>\n",
       "      <td>Its prospects depend heavily on the continuing...</td>\n",
       "      <td>2025-10-31T20:25:53.516424</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              titulo  \\\n",
       "0  Israel confirms identities of hostages' bodies...   \n",
       "1                     Who are the released hostages?   \n",
       "2  UK pledges £4m to clear land mines to help flo...   \n",
       "3  Dependants of some Gazan students can join the...   \n",
       "4               Can the Gaza ceasefire deal survive?   \n",
       "\n",
       "                                              url  \\\n",
       "0  https://www.bbc.com/news/articles/c4gj90j2g8jo   \n",
       "1  https://www.bbc.com/news/articles/cpvl9k4mw8no   \n",
       "2  https://www.bbc.com/news/articles/c9d6x02xdj6o   \n",
       "3  https://www.bbc.com/news/articles/cly91lj9y47o   \n",
       "4  https://www.bbc.com/news/articles/ckgk4x5ze3mo   \n",
       "\n",
       "                                         mini_resumo  \\\n",
       "0  The bodies were identified as those of Amiram ...   \n",
       "1  The Israeli military says 20 living hostages h...   \n",
       "2  It will allow a UN body to clear land mines an...   \n",
       "3  The decision is a reversal of the original pol...   \n",
       "4  Its prospects depend heavily on the continuing...   \n",
       "\n",
       "             timestamp_coleta  \n",
       "0  2025-10-31T20:25:53.516424  \n",
       "1  2025-10-31T20:25:53.516424  \n",
       "2  2025-10-31T20:25:53.516424  \n",
       "3  2025-10-31T20:25:53.516424  \n",
       "4  2025-10-31T20:25:53.516424  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news = pd.read_csv(OUTCSV)\n",
    "news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33959725",
   "metadata": {},
   "source": [
    "# Extração de Dados de Preços do Petróleo\n",
    "Este notebook utiliza a função `extrair_dados_petroleo` para obter os preços diários do petróleo (WTI ou Brent) em um intervalo de datas especificado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a15d220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar a função de extração\n",
    "from extrair_dados_petroleo import extrair_dados_petroleo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5889134c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir os parâmetros para a extração\n",
    "tipo_serie = \"brent\"  # Escolha entre \"wti\" ou \"brent\"\n",
    "data_inicio = \"2024-01-01\"\n",
    "data_fim = \"2025-12-31\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6545e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chamar a função para extrair os dados\n",
    "dados_petroleo = extrair_dados_petroleo(tipo_serie, data_inicio, data_fim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58bebb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ds      price\n",
      "0 2024-01-02  75.889999\n",
      "1 2024-01-03  78.250000\n",
      "2 2024-01-04  77.589996\n",
      "3 2024-01-05  78.760002\n",
      "4 2024-01-08  76.120003\n"
     ]
    }
   ],
   "source": [
    "# Visualizar as primeiras linhas do DataFrame\n",
    "print(dados_petroleo.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0030d4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo CSV salvo com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Salvar os dados em um arquivo CSV (opcional)\n",
    "dados_petroleo.to_csv(f\"precos_{tipo_serie}_{data_inicio}_to_{data_fim}.csv\", index=False)\n",
    "print(\"Arquivo CSV salvo com sucesso!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pos_unifor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
